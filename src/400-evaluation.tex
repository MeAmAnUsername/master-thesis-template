\chapter{Evaluation}
\label{chap:evaluation}

The goal of the \ac{PIE} \ac{DSL} is to reduce boilerplate and bugs.
It should do this without sacrificing runtime performance or build time performance.
From a design perspective, the \ac{PIE} \ac{DSL} should cover future use cases as much as possible, or be extendable so that new language features could be added to cover unforeseen use cases.
This chapter evaluates whether the \ac{PIE} \ac{DSL} met these goals by applying it to three case studies and possibly some performance testing.

The first case study uses Tiger, which is a small functional language.
This gives a clean example of how the PIE DSL can be used to parse, analyze and compile a Spoofax language.

The second case study applies the \ac{PIE} \ac{DSL} to database pipelines.
Explaining what that means would just be a copy of the introduction for that section, so I'll just shut up now.

The last case study uses the \ac{PIE} \ac{DSL} to test language frontends at Oracle.
These language frontends only define the syntax for a language, and are meant as compilation targets for PGX.
As a sanity check, we would like to do reparse tests: parse an example program, pretty-print it, reparse the output of pretty-printing.
This should lead to the same AST.

The last section of this chapter provides an analysis of the results.
It looks at the main goals of reducing boilerplate and performance.
It also discusses the objectives of the generality and extensibility of the language itself.


\section{Case study: Tiger}
\label{sec:evaluation__tiger}

Tiger is a small functional language introduced in \textcite{Appel1998}.\footnote{A language reference manual can be found at \url{http://www.cs.columbia.edu/~sedwards/classes/2002/w4115/tiger.pdf}. The Spoofax language specification can be found on Github: \url{https://github.com/MetaBorgCube/metaborg-tiger/tree/master/org.metaborg.lang.tiger}}
It is used here as a clean example to apply \ac{PIE}.

The goal in this use case is to run some sort of transformation.\todo{what kind of transformation? (optimization: merge integers: 1 + 2 ==> 3)}
\todo{Add example code showing input and output of transformation}
This requires parsing and analyzing the tiger program, running the transformation, pretty-printing the transformed program to a string, and finally writing the string to a file.

\subsection{Pipeline implementation}
\label{subsec:evaluation__tiger__implementation}

Figure \ref{lst:case_study_tiger_pie} shows the \ac{PIE} \ac{DSL} code for this case study.
\Ac{PIE} tasks for parsing and analyzing are generated by Spoofax when building the language project.
Writing to a file is a task in the \ac{PIE} standard library.
All that is left is writing a task that invokes the transformation and wiring everything up.

\begin{figure}
  \caption{\Ac{PIE} \ac{DSL} code for the tiger case study}
  \label{lst:case_study_tiger_pie}
  \todo{Replace with actual code}
  \begin{lstlisting}
    module tiger:optimize

    import std:writeToFile
    import mb:metaborg:spoofax3:{IStrategoTerm, invokeStrategoStrategy as invoke}

    func parse(program: supplier<string>) -> IStrategoTerm = foreign mb:metaborg:example:tiger:Parse
    func analyze(ast: IStrategoTerm) -> (AnalysisResult) = foreign mb:metaborg:example:tiger:Analyze
    func prettyprint(ast: IStrategoTerm) -> string = foreign mb:metaborg:example:tiger:PrettyPrint

    // reads a tiger file, optimizes it, and writes the result back to the same file.
    func main(file: path) -> path = {
      val ast = read file;
      val analysisResult = analyze(ast);
      val optimized = optimize(ast, analysisResult);
      writeToFile(file, prettyprint(optimized));
    }

    func optimize(ast: IStrategoTerm, analysis: AnalysisResult) -> IStrategoTerm = invoke("tiger-optimize-all")
  \end{lstlisting}
\end{figure}

\todo{Add code that is used to call the thing}

\subsection{Analysis}
\label{sec:evaluation__tiger__analysis}

\begin{figure}
  \caption{Java code equivalent to the pie code in \ref{lst:case_study_tiger_pie}.}
  \label{lst:case_study_tiger_java}
  \question{Put this here or add it as appendix?}
  \todo{Add Java code}
\end{figure}

The equivalent Java code is given in figure \ref{lst:case_study_tiger_java}.
The \ac{PIE} \ac{DSL} code uses X \ac{loc}, while the equivalent Java code uses Y \ac{loc}.
\todo{Fill in actual numbers}

\section{Case study: PGX Algorithms pipelines}
\label{sec:evaluation__database}

\begin{figure}
  \caption{An implementation of the Adamic-Adar index in Green-Marl and \ac{PGX-A}}
  \label{lst:case_study_pgx_a_adamic_adar}
  \lstinputlisting[language=Java]{code/adamic-agar.java}
  \lstinputlisting{code/adamic-agar.gm}
\end{figure}

\Ac{PGX} is a toolkit for graph analysis, both for graph algorithms and SQL-like queries.
It provides two \acp{DSL}, Green-Marl and \ac{PGX-A}.
Both can express graph algorithms.
Green-Marl is a standalone \ac{DSL} for expressing graph algorithms.
\Ac{PGX-A} is an \ac{EDSL} in Java, this makes integration with Java easier.
Examples of graph algorithms are Dijkstra's algorithm for finding the shortest path\missingref and Kruskal's algorithm for finding the minimum spanning tree\missingref.
Another example is the Adamic-Adar index of edges.
Its implementation in Green-Marl and \ac{PGX-A} can be seen in figure \ref{lst:case_study_pgx_a_adamic_adar}.
The Adamic-Adar index can be used to predict edges between nodes in a social network.
\Ac{DFS} and \ac{BFS} are considered such fundamental algorithms that they are built-in functions.
The algorithms expressed in \ac{PGX-A} are compiled to an implementation for either a single machine or shared memory \ac{PGX} runtime.



The compilation goes as follows:
\begin{enumerate}
  \item The Java code is parsed and analyzed with a Spoofax definition of Java
  \item The Java code is transformed to Green-Marl.
  \item The Green-Marl is analyzed.
  \item The Green-Marl is compiled to C, C++, Java or \ac{PL/SQL} depending on what the runtime requires.
\end{enumerate}

This section
- [done] Explain Green-Marl and PGX-A
- [done] Add example code for both
- Explain goal of this section: pipeline for PGX-A, executable as command line tool
- Give results: PIE code, equivalent Java code, supporting code (Gradle, Java classes that can't be expressed in PIE, etc)

\subsection{Introduction}
\label{sec:evaluation__database__introduction}
\todo{What are Green-Marl, PGX?}
What are we trying to do in this case study?
-> Transform programs in some language to other languages.
\\
Interesting feature: multiple language projects and backends in separate projects.

\todo{write section}

\subsection{Pipeline implementation}
\label{sec:evaluation__database__implementation}

\todo{describe implementation}

\subsection{Analysis}
\label{sec:evaluation__database__analysis}

\todo{analysis}
\todo{should include part about multiple language projects}

\section{Case study: testing pipelines}
\label{sec:evaluation__testing}

\subsection{Introduction}
\label{sec:evaluation__testing__introduction}
What are we trying to do in this case study?
-> Execute Parse and reparse tests (check that parsing and reparsing example programs works)

\todo{write section}

\subsection{Pipeline implementation}
\label{sec:evaluation__testing__implementation}

\todo{describe implementation}

\subsection{Analysis}
\label{sec:evaluation__testing__analysis}

\todo{analysis}

\section{Analysis}
\label{sec:evaluation__analysis}

Use case: user needs some pipeline.
We distinguish five categories of tasks related to writing pipeline code:
1. Implementing the initial pipeline. The user either has no pipeline at all or has an existing pipeline in some other language but decided to change to PIE.\todo
2. Changing/extending an existing pipeline. This refers to semantic changes to the pipeline: some steps need to be executed earlier, later, added or removed.\todo
3. Maintaining an existing pipeline. This refers to changes to the code that do not change the conceptual pipeline. An example is a change in the api of a dependency, so now the pipeline code needs to be updated to the new api.\todo
4. Reading the pipeline code to understand what it does, with no intention of changing it. This will happen a lot, definitely the most if we include reading of code during the other user tasks (i.e. if you want to change the pipeline in some way, you will likely need to read parts of it that do not need to be changed but that are related and need to be understood to make the change).
5. Debugging the pipeline. This is not a goal by itself, but arises as part of the other tasks. It is mentioned explicitly because it is important: being unable to properly debug the pipeline (even with just print statements) severely harms the user friendliness.



Goals of this thesis / PIE DSL: make developing in PIE more user friendly by making it easier to write tasks.
We divide this into the following subgoals, depending on the task that the user is trying to achieve:
- Increase readability. Readability is a nebulous concept, but it more or less means that we want to convey the necessary information/understanding in an as short a time as possible. "Necessary information" refers to the information that the user requires for their particular task.
Note that this does not mean that we optimize for more concise code. More concise code is only useful if it allows the user to read the code faster.
Reading the code happens in all user tasks, and is central to task 4: reading the pipeline code to understand it.
- Increase ease of writing. It should be easy and quick to write PIE DSL code.
This can be achieved by making it specific to the domain of pipelines, by including types that are relevant to pipelines and by making simple syntax for common patterns.
This is related to readability: if it is easy to read, it is likely not too long or complicated, so it would be easy to write as well.
- Make it easy to reason about. This is sort of the step between reading and writing when making a change: once the user has read the code and understands what it does, it should be easy to reason about so that it is easy to figure out what to do to make the desired changes. (Is this part of the DSL? Seems more like a task for the model that PIE is built on)
- 


Afterthought: I think of it like this:
- readability: how easy is it to read some fragment (e.g. a single task) of code and understand what it does?
- reasoning about it: once you understand what each fragment of code does, how easy is it to figure out what the program as a whole does? How easy is it to figure out what changes should be made to bring about some desired change in behavior? How easy is it to break up an understanding of what the program should do into fragments?
- Writing: once you understand what fragments there should be and how the fragments should compose to do what you want, how easy is it to write down these fragments as code or to modify the existing fragments?

Reasoning about it is mostly influenced by the model defined by PIE.
The reading and writing are where the DSL should bring improvements.


This is impossible to measure directly because it depends on the user.
There are however instrumental goals that improve usability:
- Increase the amount of information that is visible on the screen (while keeping the readability of that information the same).
- Reduce the amount of code (less code means easier maintenance ceteris paribus. Ceteris paribus does not hold, system becomes more complex, but reduction in code should outweigh increased build system complexity). (Is this just the exact same as information?)
- Increase editor services like static error reporting, showing documentation, automated imports and reference following. (PIE DSL compared to pure Java actually does the opposite: no documentation, no static checks and no following references from Java to PIE or vice versa. More static error reporting when comparing old DSL vs new DSL)
- Increase expressiveness to increase applicability. Writing several tasks or helper functions and wrappers in Java does not improve user friendliness.
- Increase applicability to various domains. PIE can't be user friendly if it doesn't work for your domain.

To evaluate whether the new PIE DSL meets these goals, we use PIE DSL to express pipelines in various domains:
- Spoofax 3 language compiler pipeline. Compiles a language specification to a language implementation.
- PGX-A program pipeline. Compiles a program from Green-Marl to C, C++, PL/SQL.
- Code editor/IDE for Tiger. Various tasks for editor actions.
- Testing pipeline: pipeline for testing the generated parser and prettyprinter of a language.
- Benchmarking: benchmarking pipelines for various things. We only want to re-execute the benchmark for tests that were changed.

These case studies will be evaluated on the following points:
- Lines of code. We express each pipeline in the old DSL, the new DSL and Java. Because we express the exact same pipeline in all these languages, they each contain the same total amount of information. This means that we can use the lines of code to compare the amount of information per line in each language. This also reduces the amount of code.
This covers the amount of information that is visible on screen (sort of) and "less code is better"
- The percentage of tasks that can be expressed in the DSL.
This covers the expressiveness of the DSL. We will also compare this percentage between case studies to gauge how applicable the PIE DSL is across domains.

We will also evaluate the differences in editor services from a theoretical standpoint. This will not be done as part of the case studies because this focusses on dynamic errors during development, and I really do not feel like writing down every time I get a static error or follow a reference.




Probably requires user studies, even then pretty hard.
Instead, we will evaluate this goal by comparing several heuristics between the new PIE DSL, the old PIE DSL, and writing everything in Java.
- Reduction in boilerplate (less is better).
  - How much code is no longer duplicated
  - How much code is no longer written at all
  - How many lines of code are there?
    Conceptually, the PIE tasks in both Java and PIE DSL express the exact same pipeline. This means that any size difference is caused by useless information (boilerplate).
    Boilerplate reduces readability because the developer now needs to spend time/thinking about whether information is relevant or not [citation needed], and because less information fits on screen, so understanding the code now requires more scrolling.
- Time to write tasks (less is better).
- readability. Highly subjective, depends on experience of the developer. (example: I am currently not experienced with the PIE api, so I find Java harder to read). In general though: PIE DSL makes it easier to read inputs, because in Java these are often not on the screen (class is too big)
- How much can be expressed. Compare other categories based on either just the tasks that are now in PIE DSL, or over whole code base (including classes /configuration that still has to be Java)





\subsection{Boilerplate reduction}
\label{subsec:evaluation__analysis__boilerplate_reduction}

The conciseness of a language depends on two things: the information redundancy and code density.
The information redundancy is a measure of how much information is repeated within a program.
For example, in the Java statement \inlinecode{int x = 5;}, the type \id{int} is redundant, as it can already be derived from the expression.
Redundancy should not be minimized entirely, because redundant information can serve to make programs more understandable and to make error checking easier.
An example are function signatures.
Often, the signature of a function can be derived from its body, but that takes time and may require very complex analysis.
Having the type signature as part of the function signature allows developers and editors to get the type of a function without analyzing or even reading the function body.
In conclusion: information redundancy should be balanced with understandability and analyzability of the code.

\Ac{loc} only captures the vertical length of a program.
There is also the horizontal length, i.e. line length.
Long lines take longer to read than short lines [citation needed].


Goal: compare both information density (how much information is put into a given amount of code) and code density (physical amount of code on screen).

While \ac{loc} is not a metric that should be optimized when writing a program, it is useful to compare boilerplate of languages.
For the number of characters, sequences of layout are counted as a single character, i.e. in the following example, the whitespace from `\{' to `p' is counted as a single character.
\begin{lstlisting}
    Class Foo {
      private final int someNumber;
\end{lstlisting}


\begin{table}
  \caption{Lines of code, number of characters and their ratios between \ac{PIE} \ac{DSL} code and the equivalent Java code.}
  \label{tbl:evaluation_analysis_loc}
  \begin{tabular}{|r||l|l|l||l|l|l|}
    \hline
    & \multicolumn{3}{|c||}{Lines of code} & \multicolumn{3}{|c|}{Number of characters} \\
    \hline
    Case study & Java & \ac{PIE} \ac{DSL} & ratio (\%) & Java & \ac{PIE} \ac{DSL} & ratio (\%) \\
    \hline
    \hline
    Tiger & A & B & $C = B / A * 100\%$ & U & V & $V / U * 100\%$ \\
    Database & D & E & $F = E / D * 100\%$ & W & X & $X / W * 100\%$ \\
    Testing & G & H & $I = H / G * 100\%$ & Y & Z & $Z / Y * 100\%$ \\
    \hline
    Average & - & - & $(C + F + I)\%$ & - & - & ... \\
    \hline
  \end{tabular}
\end{table}

The lines of code and number of characters for each case study are summarized in table \ref{tbl:evaluation_analysis_loc}.
In each case study, the ratio of lines of code is below SomeNumber (where SomeNumber should be at most 50\% or I have to check this text again), but I expect it to be around 25\%), and the ratio of characters is around SomeOtherNumber (at most 40\%, expected 25\%).
This discrepancy is due to overhead from having multiple files and actual Java boilerplate.
Because each task needs to be a separate class and classes in Java are typically written one per file, we use two files to express the tasks that were not generated by Spoofax.
This duplicates the package statement and some of the imports.

The \id{exec} method contains almost all necessary information about the task.
The exception is the parameters in case there are more than one, these are defined in the \id{Input} class.
\todo{There also injected values that are not inputs for the task, but probably shouldn't mention them here}
The boilerplate comes from elements that repeat information. These elements are:
\begin{enumerate}
  \item The class declaration repeats the class input and output type, and specifies that this is a \ac{PIE} task.
  \item The \id{Input} class specifies the parameters and their types, but then repeats them in the constructor (twice), the \id{equals}, \id{hashcode} and \id{toString} methods.
  \item The fields and the constructor repeat the dependencies on other tasks (three times)
  \item \id{getId} gives a unique identifier for the task.
  This can be derived from the task name.
\end{enumerate}

\subsection{Non-functional requirements}
\label{subsec:evaluation__analysis__non_functional_requirements}

\todo{discuss non-functional requirements}
